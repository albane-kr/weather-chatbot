{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa55ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from LSTM import WeatherLSTM\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b04b1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from TemperatureDataset import TemperatureDataset\n",
    "from functools import partial\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab06300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 562/562 [00:34<00:00, 16.19it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = TemperatureDataset('data_temp_max', 'data_temp_min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c4aadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "batch_size = 32  # You can adjust this as needed\n",
    "\n",
    "train_loader = TorchDataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = TorchDataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = TorchDataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eec8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50: 100%|██████████| 80522/80522 [05:53<00:00, 227.87it/s]\n",
      "Validation Epoch 1/50: 100%|██████████| 17255/17255 [00:28<00:00, 615.75it/s]\n",
      "Training Epoch 2/50: 100%|██████████| 80522/80522 [05:49<00:00, 230.28it/s]\n",
      "Validation Epoch 2/50: 100%|██████████| 17255/17255 [00:26<00:00, 645.45it/s]\n",
      "Training Epoch 3/50: 100%|██████████| 80522/80522 [06:23<00:00, 209.91it/s]\n",
      "Validation Epoch 3/50: 100%|██████████| 17255/17255 [00:27<00:00, 618.01it/s]\n",
      "Training Epoch 4/50: 100%|██████████| 80522/80522 [06:13<00:00, 215.58it/s]\n",
      "Validation Epoch 4/50: 100%|██████████| 17255/17255 [00:28<00:00, 606.88it/s]\n",
      "Training Epoch 5/50: 100%|██████████| 80522/80522 [06:53<00:00, 194.87it/s]\n",
      "Validation Epoch 5/50: 100%|██████████| 17255/17255 [00:24<00:00, 703.26it/s]\n",
      "Training Epoch 6/50: 100%|██████████| 80522/80522 [05:58<00:00, 224.33it/s]\n",
      "Validation Epoch 6/50: 100%|██████████| 17255/17255 [00:28<00:00, 614.85it/s]\n",
      "Training Epoch 7/50: 100%|██████████| 80522/80522 [06:09<00:00, 218.07it/s]\n",
      "Validation Epoch 7/50: 100%|██████████| 17255/17255 [00:28<00:00, 609.39it/s]\n",
      "Training Epoch 8/50: 100%|██████████| 80522/80522 [06:18<00:00, 212.78it/s]\n",
      "Validation Epoch 8/50: 100%|██████████| 17255/17255 [00:25<00:00, 666.36it/s]\n",
      "Training Epoch 9/50: 100%|██████████| 80522/80522 [05:53<00:00, 227.76it/s]\n",
      "Validation Epoch 9/50: 100%|██████████| 17255/17255 [00:24<00:00, 706.38it/s]\n",
      "Training Epoch 10/50: 100%|██████████| 80522/80522 [06:11<00:00, 216.63it/s]\n",
      "Validation Epoch 10/50: 100%|██████████| 17255/17255 [00:27<00:00, 638.43it/s]\n",
      "Training Epoch 11/50: 100%|██████████| 80522/80522 [06:13<00:00, 215.39it/s]\n",
      "Validation Epoch 11/50: 100%|██████████| 17255/17255 [00:29<00:00, 587.70it/s]\n",
      "Training Epoch 12/50: 100%|██████████| 80522/80522 [06:09<00:00, 217.81it/s]\n",
      "Validation Epoch 12/50:  65%|██████▍   | 11154/17255 [00:17<00:10, 558.15it/s]"
     ]
    }
   ],
   "source": [
    "activations_map = {\n",
    "    'relu': torch.nn.ReLU,\n",
    "    'tanh': torch.nn.Tanh,\n",
    "    'sigmoid': torch.nn.Sigmoid,\n",
    "    'leaky_relu': torch.nn.LeakyReLU,\n",
    "    'elu': torch.nn.ELU,\n",
    "    'gelu': torch.nn.GELU,\n",
    "    'selu': torch.nn.SELU,\n",
    "    'none': nn.Identity\n",
    "}\n",
    "# Prepare activations\n",
    "\n",
    "# Build and train the best model on the full training set\n",
    "sample_input, sample_target = dataset[0]\n",
    "input_dim = sample_input.shape[-1]\n",
    "output_dim = sample_target.shape[-1]\n",
    "best_model = WeatherLSTM(input_size=input_dim, output_size=output_dim, sequence_length=7, hidden_size=64, num_layers=3, dropout=0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use the full train_loader for training\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for inputs, targets in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = best_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # Save the best model\n",
    "            torch.save(best_model.state_dict(), 'best_model_temp_lstm.pt')\n",
    "torch.save(best_model.state_dict(), 'last_model_temp_lstm.pt')\n",
    "# Evaluate on the test set\n",
    "best_model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc1f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.1448\n",
      "Mean Absolute Error: 0.0912\n",
      "R2 Score: 0.3284\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, f1_score, accuracy_score\n",
    "\n",
    "# Collect all predictions and targets from the test set\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "print(best_model)\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = best_model(inputs)\n",
    "        all_preds.append(outputs.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "# Calculate regression metrics\n",
    "mse = mean_squared_error(all_targets, all_preds)\n",
    "mae = mean_absolute_error(all_targets, all_preds)\n",
    "r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfbb227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc0bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
